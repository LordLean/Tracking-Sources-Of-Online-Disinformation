{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake_news_scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNvt4RfYFOI/60PBnqndde4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LordLean/Tracking-Sources-Of-Online-Disinformation/blob/main/fake_news_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNPdsbCPlLT9"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "# nltk.download(\"stopwords\")\n",
        "# nltk.download(\"wordnet\")\n",
        "# nltk.download(\"punkt\")\n",
        "# nltk.download(\"omw\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji2ShpUfM9kB"
      },
      "source": [
        "def url_parse(url, parser=\"html.parser\"):\n",
        "  data = requests.get(url)\n",
        "  soup = BeautifulSoup(data.text, parser)\n",
        "  return soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIIvOnrSvHq"
      },
      "source": [
        "def remove_duplicates(ls):\n",
        "  return list(dict.fromkeys(ls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se8Ah902yonw"
      },
      "source": [
        "def sentence_preprocessing(sentence, custom_stopwords=[]):\n",
        "  \n",
        "  # Tokenize sentence.\n",
        "  tokenized_sentence = word_tokenize(sentence)\n",
        "\n",
        "  # Set stopwords.\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  stop_words = stop_words.union(custom_stopwords)\n",
        "  \n",
        "  # Remove stopwords and set to lowercase.\n",
        "  tokenized_sentence = [word.lower() for word in tokenized_sentence if word not in stop_words or word.isnumeric()]\n",
        "\n",
        "  return tokenized_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CUYO9NsWvo0"
      },
      "source": [
        "def headline_list_creator(ratings=[\"false\"], num_of_pages=5):\n",
        "\n",
        "  all_headlines = []\n",
        "\n",
        "  # Iterate through snopes.com claim credibility rating.\n",
        "  for rating in ratings:\n",
        "    # Iterate through snopes.com page numbers.\n",
        "    for page_num in range(1,num_of_pages+1):\n",
        "      url = 'https://www.snopes.com/fact-check/rating/{}/page/{}/'.format(rating,page_num)\n",
        "      soup = url_parse(url, \"lxml\")\n",
        "\n",
        "      # 12 is items per page of relevant content.   \n",
        "      for article in soup.find_all(\"article\", class_=\"media-wrapper\")[:12]:\n",
        "        # Extract headline text.\n",
        "        headline = article.div.h5.text\n",
        "        # Tuple object of claim credibiilty rating, headline text.\n",
        "        all_headlines.append((rating,headline))\n",
        "\n",
        "  return all_headlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyzn3eZSX68E"
      },
      "source": [
        "ratings = [\"false\",\"mostly-false\"]\n",
        "headlines = headline_list_creator(ratings, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7zXBkqmvmR_",
        "outputId": "2d9194c0-d352-40a3-8a22-319cbb407d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "source": [
        "headlines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('false', 'Is Biden Proposing ‘Biggest’ Tax Increase in US History?'),\n",
              " ('false', 'Did a Fly Land on Amy Coney Barrett?'),\n",
              " ('false', 'Are Crab Cake Oreos Real?'),\n",
              " ('false',\n",
              "  'Did Trump Say He Nominated Amy Coney Barrett Because of Her Looks?'),\n",
              " ('false', 'Does a Video Show Baby Giraffes Frolicking in a Park?'),\n",
              " ('false', 'Can Poll Workers Invalidate Ballots by Writing on Them?'),\n",
              " ('false', 'Did Kamala Harris Call American Churches ‘Propaganda Centers’?'),\n",
              " ('false', 'Did Trump Tweet ‘My Blood IS the Vaccine’?'),\n",
              " ('false',\n",
              "  'Did Lincoln Refuse to Fill a Supreme Court Vacancy Before an Election?'),\n",
              " ('false',\n",
              "  'Did Trump Say Doctors Had ‘Never Seen a Body Kill Coronavirus Like My Body’?'),\n",
              " ('false', 'Did Biden Call Trump Supporters the ‘Dregs of Society’?'),\n",
              " ('false',\n",
              "  'No, Kamala Harris Didn’t Tweet, ‘We Need to Focus on Defunding the Police’'),\n",
              " ('mostly-false',\n",
              "  'Did CDC Report ‘Majority’ of People Who Contracted COVID-19 Wore Masks?'),\n",
              " ('mostly-false', 'Did US Unemployment Reach Record Low Due to Trump?'),\n",
              " ('mostly-false', 'Did Fauci Praise Trump’s Handling of COVID-19 Pandemic?'),\n",
              " ('mostly-false', 'Did Biden Propose a Ban on Fracking?'),\n",
              " ('mostly-false', 'Does Biden Want To Abolish the Second Amendment?'),\n",
              " ('mostly-false', 'Is This ‘Biden Muslim Ad’ Real?'),\n",
              " ('mostly-false',\n",
              "  'Is the White House Gift Shop Selling a ‘Trump Defeats COVID-19’ Coin?'),\n",
              " ('mostly-false',\n",
              "  'Were ‘Doomsday Planes’ Mobilized After Trump’s COVID-19 Diagnosis?'),\n",
              " ('mostly-false', 'Did Obama Get Caught ‘Spying’ on Trump’s 2016 Campaign?'),\n",
              " ('mostly-false', 'Did Biden Disparage Troops as ‘Stupid Bastards’?'),\n",
              " ('mostly-false',\n",
              "  'Did Trump Inherit a ‘Depleted’ Stockpile of Ventilators From Obama?'),\n",
              " ('mostly-false',\n",
              "  'Did Amy Coney Barrett’s Religious Group Inspire ‘The Handmaid’s Tale’?')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMnO-KX9TrUy",
        "cellView": "form"
      },
      "source": [
        "#@title Redundant \n",
        "\n",
        "# *************************\n",
        "# url = \"https://www.snopes.com/50-hottest-urban-legends/\"\n",
        "# url = 'https://www.snopes.com/fact-check/rating/false/'\n",
        "# url = 'https://www.snopes.com/fact-check/rating/false/page/1/'\n",
        "# soup = url_parse(url, \"lxml\")\n",
        "\n",
        "\n",
        "# *************************\n",
        "# soup.find('article', class_=\"media-wrapper\")\n",
        "# soup.find('article', class_=\"media-wrapper\").div.h5.text\n",
        "# for article in soup.find_all(\"article\", class_=\"media-wrapper\"):\n",
        "#   print(article.div.h5.text)\n",
        "\n",
        "# 12 is hardcoded as only 12 new stories labelled FALSE appear per page. The remainder would be \"most popular\".\n",
        "fake_news_headlines = [article.div.h5.text for article in soup.find_all(\"article\", class_=\"media-wrapper\")[:12]]\n",
        "fake_news_headlines\n",
        "\n",
        "\n",
        "# *************************\n",
        "def headline_list_creator(num_of_pages=5):\n",
        "\n",
        "  all_headlines = []\n",
        "\n",
        "  for i in range(1,num_of_pages+1):\n",
        "    url = 'https://www.snopes.com/fact-check/rating/false/page/{}/'.format(i)\n",
        "    soup = url_parse(url, \"lxml\")\n",
        "    fake_news_headlines = [article.div.h5.text for article in soup.find_all(\"article\", class_=\"media-wrapper\")[:12]]    \n",
        "    all_headlines.append(fake_news_headlines)\n",
        "\n",
        "  return all_headlines\n",
        "\n",
        "\n",
        "def flatten(list):\n",
        "  flattened = [item for sublist in list for item in sublist]\n",
        "  return flattened\n",
        "\n",
        "# *************************"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv6m9KSXFrfh",
        "outputId": "3e08005a-043d-403e-eb13-6facc2c46383",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "eg = 'This is the greatest tv showing of all timings in cared places this 300'\n",
        "\n",
        "print(sentence_preprocessing(eg))\n",
        "\n",
        "\n",
        "keywords = []\n",
        "\n",
        "syns = wn.synsets(\"Killing\")\n",
        "for synset in syns:\n",
        "  for lemma in synset.lemmas(lang=\"eng\"):\n",
        "    keywords.append(lemma.name())\n",
        "\n",
        "remove_duplicates(keywords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'greatest', 'tv', 'showing', 'timings', 'cared', 'places', '300']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['killing',\n",
              " 'violent_death',\n",
              " 'kill',\n",
              " 'putting_to_death',\n",
              " 'cleanup',\n",
              " 'shoot_down',\n",
              " 'defeat',\n",
              " 'vote_down',\n",
              " 'vote_out',\n",
              " 'stamp_out',\n",
              " 'toss_off',\n",
              " 'pop',\n",
              " 'bolt_down',\n",
              " 'belt_down',\n",
              " 'pour_down',\n",
              " 'down',\n",
              " 'drink_down',\n",
              " 'obliterate',\n",
              " 'wipe_out',\n",
              " 'sidesplitting']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    }
  ]
}